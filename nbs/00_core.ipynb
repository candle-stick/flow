{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp domain.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from dataclasses import dataclass, field\n",
    "from typing import (\n",
    "    Set, Optional, Dict, Any, List, Tuple\n",
    ")\n",
    "from functools import partial\n",
    "from dpcontracts import require, types, ensure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Model \n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass(frozen=True, unsafe_hash=True)\n",
    "class Task:\n",
    "    \"\"\"Represents an Airflow Task\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    task_type: str\n",
    "    dependencies: Optional[Set[str]] = field(default=None, hash=False)\n",
    "    parameters: Optional[dict] = field(default=None, hash=False, repr=False)\n",
    "    config: Optional[dict] = field(default=None, hash=False, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.name != \"\"\n",
    "        assert self.task_type != \"\"\n",
    "    \n",
    "    def todict(self):\n",
    "        operator, config_args = list(self.config.get(\"operator\").items()).pop()\n",
    "        args = {**config_args, **self.parameters}\n",
    "        return {\"name\": self.name,\n",
    "                \"dependencies\": self.dependencies,\n",
    "                \"type\": operator,\n",
    "                \"args\": args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'task3',\n",
       " 'dependencies': [],\n",
       " 'type': 'BashOperator',\n",
       " 'args': {'task_id': 'sleep', 'bash_command': 'sleep', 'retries': 3}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = Task(name = 'task3',\n",
    "            task_type = 'ssh',\n",
    "            dependencies = [],\n",
    "            parameters = {'task_id': 'sleep', 'bash_command': 'sleep', 'retries': 3},\n",
    "            config = {\n",
    "                'operator': {\n",
    "                    'BashOperator': {'task_id': 'sleep', \n",
    "                                     'bash_command': 'sleep',\n",
    "                                     'retries': 3}},\n",
    "                    'imports': {'airflow.operators.bash_operator': ['BashOperator']}})\n",
    "\n",
    "task.todict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class WorkflowDefinition:\n",
    "    \"\"\"Data holder and validation for YAML workflow file\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    tasks: Dict[str, dict]\n",
    "    subtasks: Dict[str, Dict[str, dict]]\n",
    "    default_args: Dict[str, Any]\n",
    "    dag_args: Dict[str, Any]\n",
    "        \n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate workflow inputs\n",
    "        \"\"\"\n",
    "        assert len([*self.tasks]) >= 1, \\\n",
    "            \"Workflow data must have at least one task\"\n",
    "        assert all(isinstance(task, str) for task in [*self.tasks]) and \\\n",
    "               not any(task == \"\" for task in [*self.tasks]), \\\n",
    "            \"Workflow tasks must have valid string names\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def build(\n",
    "        name: str, \n",
    "        tasks: Dict[str, dict],\n",
    "        subtasks: Dict[str, dict],\n",
    "        default_args: Dict[str, Any],\n",
    "        dag_args: Dict[str, Any],\n",
    "        **kwargs,\n",
    "    ) -> \"WorkflowDefinition\":\n",
    "        \"\"\"Builds Workflow definition from YAML workflow file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            workflow name\n",
    "        tasks : Dict[str, dict]\n",
    "            workflow tasks used to create primary DAG\n",
    "        subtasks : Dict[str, tasks]\n",
    "            worfklow sub-tasks used to create sub DAGs\n",
    "        default_args : Dict[str, Any]\n",
    "            default Airflow DAG arguments\n",
    "        dag_args : Dict[str, Any]\n",
    "            custom Airflow DAG arguments\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        WorkflowDefinition : Dict[str, Dict[str, dict]]\n",
    "            attributes[name, tasks, subtasks, default_args, dag_args]\n",
    "        \"\"\"\n",
    "    \n",
    "        return WorkflowDefinition(name, tasks, subtasks, default_args, dag_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Data holder for YAML Airflow configuration file.\n",
    "    Holds mapping of user-defined tasks to Airflow Operators\n",
    "    \"\"\"\n",
    "\n",
    "    task_types: Dict[str, dict]\n",
    "\n",
    "    @staticmethod\n",
    "    @require(\n",
    "        \"Configuration file must have at least one task-type\",\n",
    "        lambda args: len([*args.task_types]) >= 1,\n",
    "    )\n",
    "    @require(\n",
    "        \"Configuration file must have valid task-type names\",\n",
    "        lambda args: all(\n",
    "            isinstance(task_type, str) for task_type in [*args.task_types]\n",
    "        )\n",
    "        and not any(task_type == \"\" for task_type in [*args.task_types]),\n",
    "    )\n",
    "    @types(task_types=dict)\n",
    "    def build(task_types: Dict[str, dict], **kwargs) -> \"Config\":\n",
    "        return Config(task_types=task_types)\n",
    "\n",
    "    @types(task_type=str)\n",
    "    @require(\n",
    "        \"User specified task type must be in configuration file\",\n",
    "        lambda args: args.task_type in [*args.self.task_types],\n",
    "    )\n",
    "    def fetch(self, task_type: str) -> \"Config\":\n",
    "        \"\"\"Fetch specific task configuration from configuration file.\n",
    "\n",
    "        Config.task_types reduced to single task_type requested.\n",
    "        \"\"\"\n",
    "        task_dict = {task_type: self.task_types.get(task_type)}\n",
    "        return Config(task_types=task_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class Workflow:\n",
    "    \"\"\"Defines how to construct a Workflow object from Workflow Definition\"\"\"\n",
    "\n",
    "    name: str\n",
    "    config: Dict[str, dict]\n",
    "    tasks: Optional[List[Task]] = None\n",
    "\n",
    "    def build(self, tasks) -> \"Workflow\":\n",
    "        # Extract Tasks from workflow definition\n",
    "        compose = partial(self.compose_task, self.config)\n",
    "        tasks = [\n",
    "            compose((name, definition))\n",
    "            for name, definition in tasks.items()\n",
    "        ]\n",
    "        return Workflow(self.name, self.config, tasks)\n",
    "    \n",
    "\n",
    "    @ensure(\"Task type not found\", lambda args, result: result.config is not None)\n",
    "    def compose_task(self, config: Dict[str, dict], task: Tuple[str, dict]) -> Task:\n",
    "        # Build Action objects with type specific settings from config file\n",
    "        name, definition = task\n",
    "        task_type = definition.get(\"type\")\n",
    "        return Task(name, \n",
    "                    task_type,\n",
    "                    definition.get(\"dependencies\"),\n",
    "                    definition.get(\"parameters\"),\n",
    "                    config.get(task_type))\n",
    "    \n",
    "    #TODO\n",
    "    def imports(self):\n",
    "        \"\"\"Extract a unique set of module imports required by the Workflow Definition.\n",
    "        \"\"\"\n",
    "        all_imports = [task.config.get(\"imports\") for task in self.tasks]\n",
    "        print(all_imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 02_graph.ipynb.\n",
      "Converted 03_templates.ipynb.\n",
      "Converted 04_adapters.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted software_architecture.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
