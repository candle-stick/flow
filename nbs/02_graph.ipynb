{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp services.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from dataclasses import dataclass, asdict\n",
    "from functools import partial\n",
    "from typing import Any, List, Tuple, Iterator, Optional, Dict\n",
    "from yaml import safe_load\n",
    "from networkx import (\n",
    "    DiGraph,\n",
    "    compose,\n",
    "    is_directed_acyclic_graph,\n",
    "    relabel_nodes,\n",
    "    topological_sort,\n",
    "    descendants,\n",
    "    number_of_nodes,\n",
    ")\n",
    "from flow.domain.model import Task, Config, WorkflowDefinition, Workflow\n",
    "from flow.adapters.templater import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Graph\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Input Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from networkx.drawing.nx_pydot import to_pydot\n",
    "\n",
    "\n",
    "def view_pydot(pdot):\n",
    "    plt = Image(pdot.create_png())\n",
    "    display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def render(templater: Templater, template, task: dict) -> str:\n",
    "    return templater.render(template = template, content = task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2 = BashOperator(\n",
      "    task_id = \"task2\",\n",
      "    task_id = sleep,\n",
      "    bash_command = sleep,\n",
      "    retries = 3,\n",
      "    dag = dag\n",
      ")\n",
      "\n",
      "task1 >> task2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default configuration for Airflow Tasks and Oozie Actions\n",
    "config = {\n",
    "    \"ssh\": {\n",
    "        \"BashOperator\": {\n",
    "            \"task_id\": \"sleep\",\n",
    "            \"bash_command\": \"sleep\",\n",
    "            \"retries\": 3\n",
    "        },\n",
    "        \"imports\": {\n",
    "            # module: List[object]\n",
    "            \"airflow.operators.bash_operator\": [\"BashOperator\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "workflow = {\n",
    "    \"name\": 'airflow',\n",
    "    \"imports\": '', #For each task, search the config for required imports and then merge\n",
    "    \"default_args\": {},\n",
    "    \"dag_args\": {},\n",
    "    \"tasks\": {},\n",
    "}\n",
    "\n",
    "# task extract operator data from config.\n",
    "task = {\n",
    "    \"name\": \"task2\",\n",
    "    \"dependencies\": [\"task1\"],\n",
    "    \"type\": \"BashOperator\",\n",
    "    \"args\": {\n",
    "        \"task_id\": \"sleep\",\n",
    "        \"bash_command\": \"sleep\",\n",
    "        \"retries\": 3      \n",
    "    }\n",
    "}\n",
    "\n",
    "# scaffold extract import data from config\n",
    "scaffold = {\n",
    "    \"metadata\": \"\",\n",
    "    \"imports\": {\n",
    "        'module1': ['object1', 'object2'],\n",
    "        \"module2\": [],\n",
    "        \"module3\": ['object3'],\n",
    "        \"module3\": [\"*\"],\n",
    "    },\n",
    "    \"default_args\": {\n",
    "        'owner': \"'airflow'\",\n",
    "        'depends_on_past': False,\n",
    "        'start_date': 'datetime(2018, 5, 26)',\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': 'timedelta(minutes=5)',\n",
    "    },\n",
    "    \"dag_args\": {\n",
    "        'description': \"'A simple tutorial DAG'\",\n",
    "        'schedule_interval': \"'@daily'\",\n",
    "    },\n",
    "}\n",
    "\n",
    "templater = JinjaTemplater()\n",
    "\n",
    "# Test BashOperator Task is rendered correctly\n",
    "print(render(templater, 'operator.txt', task))\n",
    "\n",
    "# Test Scaffold in rendered correctly\n",
    "# print(render(templater, 'scaffold.txt', scaffold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTemplater(list):\n",
    "    \n",
    "    def get_template(self, filename):\n",
    "        self.append(('GET', filename))\n",
    "        \n",
    "    def render(self, template, content):\n",
    "        self.get_template(template)\n",
    "        self.append(('RENDER', template, content))\n",
    "\n",
    "def test_render_airflow_operator():\n",
    "    templater = FakeTemplater()\n",
    "    template = 'operator.txt'\n",
    "    task = {\n",
    "        \"name\": \"task2\",\n",
    "        \"dependencies\": [\"task1\"],\n",
    "        \"type\": \"BashOperator\",\n",
    "        \"args\": {\n",
    "            \"task_id\": \"sleep\",\n",
    "            \"bash_command\": \"sleep\",\n",
    "            \"retries\": 3      \n",
    "        }\n",
    "    }\n",
    "    render(templater, template, task)\n",
    "    assert templater == [('GET', 'operator.txt'), \n",
    "                         ('RENDER', 'operator.txt', task)]\n",
    "\n",
    "def test_render_airflow_scaffold():\n",
    "    templater = FakeTemplater()\n",
    "    template = 'scaffold.txt'\n",
    "    scaffold = {\n",
    "        \"imports\": {\n",
    "            \"module1\": ['object1', 'object2'],\n",
    "            \"module2\": [],\n",
    "            \"module3\": ['object3'],\n",
    "            \"module3\": [\"*\"],\n",
    "        }\n",
    "    }\n",
    "    render(templater, template, scaffold)\n",
    "    assert templater == [('GET', template), ('RENDER', template, scaffold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_render_airflow_operator()\n",
    "test_render_airflow_scaffold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow and Config data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load(read, build, path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        data = read(f)\n",
    "    return build(**data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def build_workflow(builder, configuration, name, tasks) -> \"Workflow\":\n",
    "    \"\"\"Build workflows from configuration and definition tasks/subtasks\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    builder\n",
    "        Workflow class with `build` method\n",
    "    configuration\n",
    "        Workflow configuration\n",
    "    name\n",
    "        Workflow name\n",
    "    tasks\n",
    "        Workflow tasks\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Workflow\n",
    "        Workflow object with list of Tasks used to generate graph\n",
    "    \"\"\"\n",
    "    return builder(name, configuration.task_types).build(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def build_graph(workflow: Workflow) -> DiGraph:\n",
    "    # Add all tasks to a map of string -> task\n",
    "    task_dict = {task.name: task for task in workflow.tasks}\n",
    "    task_names = task_dict.keys()\n",
    "\n",
    "    # DAG of the workflow in its raw/un-optimized state\n",
    "    input_graph = DiGraph()\n",
    "\n",
    "    # Add all dependencies as edges\n",
    "    dependencies = [\n",
    "        (task_dict[dependency], task)\n",
    "        for task in workflow.tasks\n",
    "        for dependency in task.dependencies\n",
    "    ]\n",
    "    input_graph.add_edges_from(dependencies)\n",
    "\n",
    "    # Add all the tasks as vertices.\n",
    "    input_graph.add_nodes_from(workflow.tasks)\n",
    "\n",
    "    # Make sure all dependencies have an associated task\n",
    "    dep_tasks = set(\n",
    "        [\n",
    "            dependency\n",
    "            for task in workflow.tasks\n",
    "            for dependency in task.dependencies\n",
    "        ]\n",
    "    )\n",
    "    if not dep_tasks.issubset(task_names):\n",
    "        dep = dep_tasks - task_names\n",
    "        raise WorkflowGraphError(f\"Missing task for dependencies: {dep}\")\n",
    "\n",
    "    return input_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(G: DiGraph, output_dir: str = None):\n",
    "    # re-label node with action names\n",
    "    mapping = {action: action.name for action in G.nodes()}\n",
    "    g = relabel_nodes(G, mapping)\n",
    "    # save image\n",
    "    pdot = to_pydot(g)\n",
    "\n",
    "    if output_dir:\n",
    "        pdot.write_png(output_dir)\n",
    "\n",
    "    return pdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hide\n",
    "\n",
    "# # Visualize the graph\n",
    "# viz = draw(workflow_graph)\n",
    "# view_pydot(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Workflow from Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_imports(tasks):\n",
    "    \"\"\"Extracts imports from workflow tasks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tasks : List[Task]\n",
    "        workflow tasks\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    imports : Dict[module, Set[objects]]\n",
    "        Set of object imports for every module\n",
    "        \n",
    "    >>> {'airflow.operators.bash_operator': {'BashOperator'},\n",
    "         'airflow.operators.subdag_operator': {'SubDagOperator'}}\n",
    "    \"\"\"\n",
    "    imports = {}\n",
    "    for task in tasks:\n",
    "        for (module, objs) in task.imports().items():\n",
    "            # initialiase empty set for every module\n",
    "            imports.setdefault(module, set())\n",
    "            # add objects into set of imports for given module\n",
    "            for obj in objs:\n",
    "                imports[module].add(obj)\n",
    "    return imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Load workflow definition and configuration\n",
    "load_definition = partial(load, safe_load, WorkflowDefinition.build)\n",
    "load_configuration = partial(load, safe_load, Config.build)\n",
    "\n",
    "workflow_definition = load_definition('../temp/workflow.yaml')\n",
    "workflow_configuration = load_configuration('../temp/config.yaml')\n",
    "\n",
    "# Workflow builder\n",
    "build = partial(build_workflow, \n",
    "                Workflow,\n",
    "                workflow_configuration)\n",
    "\n",
    "# Templater instantiation\n",
    "templater = JinjaTemplater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build workflow object and derive graph\n",
    "workflow = build(workflow_definition.name, workflow_definition.tasks)\n",
    "\n",
    "workflow_graph = build_graph(workflow)\n",
    "\n",
    "# Render Scaffold from workflow definition \n",
    "render_scaffold = partial(render, templater, 'scaffold.txt')\n",
    "scaffold = {\"name\": workflow.name,\n",
    "            \"imports\": get_task_imports(workflow.tasks),\n",
    "            \"default_args\": workflow_definition.default_args,\n",
    "            \"dag_args\": workflow_definition.dag_args}\n",
    "\n",
    "rendered_scaffold = render_scaffold(scaffold)\n",
    "\n",
    "# Render operator from workflow graph\n",
    "render_operator = partial(render, templater, 'operator.txt')\n",
    "rendered_tasks = [render_operator(task.todict())\n",
    "                  for task in topological_sort(workflow_graph)]\n",
    "\n",
    "# Combine Scaffold, Operator and SubDAGs\n",
    "dag_definition = '\\n'.join([rendered_scaffold, '\\n']+rendered_tasks)\n",
    "\n",
    "\n",
    "# Build subworkflows and derives graph\n",
    "subworkflows = {name : build(name, subtask) \n",
    "                for (name, subtask) \n",
    "                in workflow_definition.subtasks.items()}\n",
    "\n",
    "subworkflow_graphs = {name : build_graph(subworkflow)\n",
    "                      for (name, subworkflow)\n",
    "                      in subworkflows.items()}\n",
    "\n",
    "# Render SubDags from subworkflow graph\n",
    "render_subtask = partial(render, templater, 'subtask.txt')\n",
    "\n",
    "render_subtasks = {}\n",
    "for (name, subworkflow_graph) in subworkflow_graphs.items():\n",
    "    render_subtasks.setdefault(name, [])\n",
    "    for subtask in topological_sort(subworkflow_graph):\n",
    "        render_subtasks[name].append(render_subtask(subtask.todict()))\n",
    "        \n",
    "render_sub_scaffold = partial(render, templater, 'subtask-scaffold.txt')\n",
    "\n",
    "scaffolds = []\n",
    "\"\"\"\n",
    "[{'name': 'snapshot-entity1',\n",
    "  'tasks': {'name': 'task4',\n",
    "            'dependencies': ['task3'],\n",
    "            'type': 'SubDagOperator',\n",
    "            'args': {'retries': 3}},\n",
    "  'imports': {'airflow.operators.bash_operator': {'BashOperator'}}},\n",
    " {'name': 'delta-entity1',\n",
    "  'tasks': {'name': 'task4',\n",
    "            'dependencies': ['task3'],\n",
    "            'type': 'SubDagOperator',\n",
    "            'args': {'retries': 3}},\n",
    "  'imports': {'airflow.operators.bash_operator': {'BashOperator'}}}\n",
    "]\n",
    "\"\"\"\n",
    "scaffolds = [dict(zip(\n",
    "                ('name', 'tasks', 'imports', 'default_args', 'dag_args'), \n",
    "                (name, render_subtasks.get(name), get_task_imports(subworkflows.get(name).tasks),\n",
    "                 workflow_definition.default_args, workflow_definition.dag_args)))\n",
    "             for name, tasks \n",
    "             in render_subtasks.items()]\n",
    "\n",
    "rendered_subworkflows = [render_sub_scaffold(subtask) for subtask in scaffolds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "from airflow import DAG\n",
      "\n",
      "from airflow.contrib.operators.ssh_operator import SSHOperator\n",
      "from airflow.contrib.hooks import SSHHook\n",
      "from airflow.operators.subdag_operator import SubDagOperator\n",
      "\n",
      "DAG_NAME = \"example-dag\"\n",
      "\n",
      "DEFAULT_ARGS = {\n",
      "    owner: 'airflow',\n",
      "    depends_on_past: False,\n",
      "    start_date: datetime(2018, 5, 26),\n",
      "    retries: 1,\n",
      "    retry_delay: timedelta(minutes=5),\n",
      "}\n",
      "\n",
      "dag = DAG(\n",
      "    default_args = DEFAULT_ARGS,\n",
      "    description = 'An example DAG',\n",
      "    schedule_interval = '@daily',\n",
      ")\n",
      "\n",
      "\n",
      "task1 = SSHOperator(\n",
      "    task_id = \"task1\",\n",
      "    ssh_hook = SSHHook(ssh_conn_id=dev_config[\"emr_con_id\"]),\n",
      "    command = 'some command',\n",
      "    timeout = 30,\n",
      "    retries = 4,\n",
      "    retry_delay = timedelta(seconds=45),\n",
      "    dag = dag\n",
      ")\n",
      "\n",
      "\n",
      "task2 = SSHOperator(\n",
      "    task_id = \"task2\",\n",
      "    ssh_hook = SSHHook(ssh_conn_id=dev_config[\"emr_con_id\"]),\n",
      "    command = 'some command',\n",
      "    timeout = 30,\n",
      "    retries = 4,\n",
      "    retry_delay = timedelta(seconds=45),\n",
      "    dag = dag\n",
      ")\n",
      "\n",
      "task1 >> task2\n",
      "\n",
      "task3 = SSHOperator(\n",
      "    task_id = \"task3\",\n",
      "    ssh_hook = SSHHook(ssh_conn_id=dev_config[\"emr_con_id\"]),\n",
      "    command = 'some command',\n",
      "    timeout = 30,\n",
      "    retries = 3,\n",
      "    retry_delay = timedelta(seconds=45),\n",
      "    dag = dag\n",
      ")\n",
      "\n",
      "task2 >> task3\n",
      "\n",
      "snapshot_entity1 = SubDagOperator(\n",
      "    task_id = \"snapshot_entity1\",\n",
      "    retries = 3,\n",
      "    subdag = dag_snapshot_entity1(DAG_NAME, \"snapshot_entity1\"),\n",
      "    dag = dag\n",
      ")\n",
      "\n",
      "task3 >> snapshot_entity1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dag_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "from airflow.models import Variable\n",
      "form airflow.utils.dates import days_ago\n",
      "from airflow import DAG\n",
      "\n",
      "from airflow.contrib.operators.ssh_operator import SSHOperator\n",
      "from airflow.contrib.hooks import SSHHook\n",
      "\n",
      "def dag_snapshot_entity1(dag_id):\n",
      "\n",
      "    DEFAULT_ARGS = {\n",
      "        owner: 'airflow',\n",
      "        depends_on_past: False,\n",
      "        start_date: datetime(2018, 5, 26),\n",
      "        retries: 1,\n",
      "        retry_delay: timedelta(minutes=5),\n",
      "    }\n",
      "\n",
      "    dag = DAG(\n",
      "        default_args = DEFAULT_ARGS,\n",
      "        description = 'An example DAG',\n",
      "        schedule_interval = '@daily',\n",
      "    )\n",
      "    \n",
      "    task1 = SSHOperator(\n",
      "      ssh_hook = SSHHook(ssh_conn_id=dev_config[\"emr_con_id\"]),\n",
      "      command = 'some command',\n",
      "      timeout = 30,\n",
      "      retries = 3,\n",
      "      retry_delay = timedelta(seconds=45),\n",
      "      dag = dag)\n",
      "\n",
      "  \n",
      "    task2 = SSHOperator(\n",
      "      ssh_hook = SSHHook(ssh_conn_id=dev_config[\"emr_con_id\"]),\n",
      "      command = 'some command',\n",
      "      timeout = 30,\n",
      "      retries = 3,\n",
      "      retry_delay = timedelta(seconds=45),\n",
      "      dag = dag)\n",
      "\n",
      "    task1 >> task2\n",
      "  \n",
      "    return dag\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "from airflow.models import Variable\n",
      "form airflow.utils.dates import days_ago\n",
      "from airflow import DAG\n",
      "\n",
      "from airflow.contrib.operators.ssh_operator import SSHOperator\n",
      "from airflow.contrib.hooks import SSHHook\n",
      "\n",
      "def dag_delta_entity1(dag_id):\n",
      "\n",
      "    DEFAULT_ARGS = {\n",
      "        owner: 'airflow',\n",
      "        depends_on_past: False,\n",
      "        start_date: datetime(2018, 5, 26),\n",
      "        retries: 1,\n",
      "        retry_delay: timedelta(minutes=5),\n",
      "    }\n",
      "\n",
      "    dag = DAG(\n",
      "        default_args = DEFAULT_ARGS,\n",
      "        description = 'An example DAG',\n",
      "        schedule_interval = '@daily',\n",
      "    )\n",
      "    \n",
      "    task1 = SSHOperator(\n",
      "      ssh_hook = SSHHook(ssh_conn_id=dev_config[\"emr_con_id\"]),\n",
      "      command = 'some command',\n",
      "      timeout = 30,\n",
      "      retries = 3,\n",
      "      retry_delay = timedelta(seconds=45),\n",
      "      dag = dag)\n",
      "\n",
      "  \n",
      "    task2 = SSHOperator(\n",
      "      ssh_hook = SSHHook(ssh_conn_id=dev_config[\"emr_con_id\"]),\n",
      "      command = 'some command',\n",
      "      timeout = 30,\n",
      "      retries = 3,\n",
      "      retry_delay = timedelta(seconds=45),\n",
      "      dag = dag)\n",
      "\n",
      "    task1 >> task2\n",
      "  \n",
      "    return dag\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subtask in rendered_subworkflows:\n",
    "    print(subtask + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
