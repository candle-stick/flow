{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp services.all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from dataclasses import dataclass, asdict\n",
    "from functools import partial\n",
    "from typing import Any, List, Tuple, Iterator, Optional, Dict\n",
    "from yaml import safe_load\n",
    "from networkx import (\n",
    "    DiGraph,\n",
    "    compose,\n",
    "    is_directed_acyclic_graph,\n",
    "    relabel_nodes,\n",
    "    topological_sort,\n",
    "    descendants,\n",
    "    number_of_nodes,\n",
    ")\n",
    "from flow.domain.model import Task, Config, WorkflowDefinition, Workflow\n",
    "from flow.adapters.templater import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Graph\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Input Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from networkx.drawing.nx_pydot import to_pydot\n",
    "\n",
    "\n",
    "def view_pydot(pdot):\n",
    "    plt = Image(pdot.create_png())\n",
    "    display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def render(templater: Templater, template, task: dict) -> str:\n",
    "    return templater.render(template = template, content = task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default configuration for Airflow Tasks and Oozie Actions\n",
    "config = {\n",
    "    \"ssh\": {\n",
    "        \"BashOperator\": {\n",
    "            \"task_id\": \"sleep\",\n",
    "            \"bash_command\": \"sleep\",\n",
    "            \"retries\": 3\n",
    "        },\n",
    "        \"imports\": {\n",
    "            # module: List[object]\n",
    "            \"airflow.operators.bash_operator\": [\"BashOperator\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "workflow = {\n",
    "    \"name\": 'airflow',\n",
    "    \"imports\": '', #For each task, search the config for required imports and then merge\n",
    "    \"default_args\": {},\n",
    "    \"dag_args\": {},\n",
    "    \"tasks\": {},\n",
    "}\n",
    "\n",
    "# task extract operator data from config.\n",
    "task = {\n",
    "    \"name\": \"task2\",\n",
    "    \"dependencies\": [\"task1\"],\n",
    "    \"type\": \"BashOperator\",\n",
    "    \"args\": {\n",
    "        \"task_id\": \"sleep\",\n",
    "        \"bash_command\": \"sleep\",\n",
    "        \"retries\": 3      \n",
    "    }\n",
    "}\n",
    "\n",
    "# scaffold extract import data from config\n",
    "scaffold = {\n",
    "    \"metadata\": \"\",\n",
    "    \"imports\": {\n",
    "        'module1': ['object1', 'object2'],\n",
    "        \"module2\": [],\n",
    "        \"module3\": ['object3'],\n",
    "        \"module3\": [\"*\"],\n",
    "    },\n",
    "    \"default_args\": {\n",
    "        'owner': \"'airflow'\",\n",
    "        'depends_on_past': False,\n",
    "        'start_date': 'datetime(2018, 5, 26)',\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': 'timedelta(minutes=5)',\n",
    "    },\n",
    "    \"dag_args\": {\n",
    "        'description': \"'A simple tutorial DAG'\",\n",
    "        'schedule_interval': \"'@daily'\",\n",
    "    },\n",
    "}\n",
    "\n",
    "templater = JinjaTemplater()\n",
    "\n",
    "# Test BashOperator Task is rendered correctly\n",
    "print(render(templater, 'operator.txt', task))\n",
    "\n",
    "# Test Scaffold in rendered correctly\n",
    "# print(render(templater, 'scaffold.txt', scaffold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeTemplater(list):\n",
    "    \n",
    "    def get_template(self, filename):\n",
    "        self.append(('GET', filename))\n",
    "        \n",
    "    def render(self, template, content):\n",
    "        self.get_template(template)\n",
    "        self.append(('RENDER', template, content))\n",
    "\n",
    "def test_render_airflow_operator():\n",
    "    templater = FakeTemplater()\n",
    "    template = 'operator.txt'\n",
    "    task = {\n",
    "        \"name\": \"task2\",\n",
    "        \"dependencies\": [\"task1\"],\n",
    "        \"type\": \"BashOperator\",\n",
    "        \"args\": {\n",
    "            \"task_id\": \"sleep\",\n",
    "            \"bash_command\": \"sleep\",\n",
    "            \"retries\": 3      \n",
    "        }\n",
    "    }\n",
    "    render(templater, template, task)\n",
    "    assert templater == [('GET', 'operator.txt'), \n",
    "                         ('RENDER', 'operator.txt', task)]\n",
    "\n",
    "def test_render_airflow_scaffold():\n",
    "    templater = FakeTemplater()\n",
    "    template = 'scaffold.txt'\n",
    "    scaffold = {\n",
    "        \"imports\": {\n",
    "            \"module1\": ['object1', 'object2'],\n",
    "            \"module2\": [],\n",
    "            \"module3\": ['object3'],\n",
    "            \"module3\": [\"*\"],\n",
    "        }\n",
    "    }\n",
    "    render(templater, template, scaffold)\n",
    "    assert templater == [('GET', template), ('RENDER', template, scaffold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_render_airflow_operator()\n",
    "test_render_airflow_scaffold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow and Config data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load(read, build, path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        data = read(f)\n",
    "    return build(**data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def build_workflow(builder, configuration, name, tasks) -> \"Workflow\":\n",
    "    \"\"\"Build workflows from configuration and definition tasks/subtasks\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    builder\n",
    "        Workflow class with `build` method\n",
    "    configuration\n",
    "        Workflow configuration\n",
    "    name\n",
    "        Workflow name\n",
    "    tasks\n",
    "        Workflow tasks\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Workflow\n",
    "        Workflow object with list of Tasks used to generate graph\n",
    "    \"\"\"\n",
    "    return builder(name, configuration.task_types).build(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def build_graph(workflow: Workflow) -> DiGraph:\n",
    "    # Add all tasks to a map of string -> task\n",
    "    task_dict = {task.name: task for task in workflow.tasks}\n",
    "    task_names = task_dict.keys()\n",
    "\n",
    "    # DAG of the workflow in its raw/un-optimized state\n",
    "    input_graph = DiGraph()\n",
    "\n",
    "    # Add all dependencies as edges\n",
    "    dependencies = [\n",
    "        (task_dict[dependency], task)\n",
    "        for task in workflow.tasks\n",
    "        for dependency in task.dependencies\n",
    "    ]\n",
    "    input_graph.add_edges_from(dependencies)\n",
    "\n",
    "    # Add all the tasks as vertices.\n",
    "    input_graph.add_nodes_from(workflow.tasks)\n",
    "\n",
    "    # Make sure all dependencies have an associated task\n",
    "    dep_tasks = set(\n",
    "        [\n",
    "            dependency\n",
    "            for task in workflow.tasks\n",
    "            for dependency in task.dependencies\n",
    "        ]\n",
    "    )\n",
    "    if not dep_tasks.issubset(task_names):\n",
    "        dep = dep_tasks - task_names\n",
    "        raise WorkflowGraphError(f\"Missing task for dependencies: {dep}\")\n",
    "\n",
    "    return input_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(G: DiGraph, output_dir: str = None):\n",
    "    # re-label node with action names\n",
    "    mapping = {action: action.name for action in G.nodes()}\n",
    "    g = relabel_nodes(G, mapping)\n",
    "    # save image\n",
    "    pdot = to_pydot(g)\n",
    "\n",
    "    if output_dir:\n",
    "        pdot.write_png(output_dir)\n",
    "\n",
    "    return pdot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hide\n",
    "\n",
    "# # Visualize the graph\n",
    "# viz = draw(workflow_graph)\n",
    "# view_pydot(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Workflow from Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Load workflow definition and configuration\n",
    "load_definition = partial(load, safe_load, WorkflowDefinition.build)\n",
    "load_configuration = partial(load, safe_load, Config.build)\n",
    "\n",
    "workflow_definition = load_definition('../temp/workflow.yaml')\n",
    "workflow_configuration = load_configuration('../temp/config.yaml')\n",
    "\n",
    "# Build workflow object and derive graph\n",
    "build = partial(build_workflow, \n",
    "                Workflow,\n",
    "                workflow_configuration)\n",
    "\n",
    "workflow = build(workflow_definition.name, workflow_definition.tasks)\n",
    "\n",
    "workflow_graph = build_graph(workflow)\n",
    "\n",
    "# Render Scaffold from workflow definition \n",
    "templater = JinjaTemplater()\n",
    "\n",
    "render_scaffold = partial(render, templater, 'scaffold.txt')\n",
    "# import = set( all imports from workflow )\n",
    "scaffold = {\"imports\": {},\n",
    "            \"default_args\": workflow_definition.default_args,\n",
    "            \"dag_args\": workflow_definition.dag_args}\n",
    "\n",
    "rendered_scaffold = render_scaffold(scaffold)\n",
    "\n",
    "# Render operator from workflow graph\n",
    "render_operator = partial(render, templater, 'operator.txt')\n",
    "rendered_tasks = [render_operator(task.todict())\n",
    "                  for task in topological_sort(workflow_graph)]\n",
    "\n",
    "# Combine Scaffold, Operator and SubDAGs\n",
    "dag_definition = '\\n'.join([rendered_scaffold, '\\n']+rendered_tasks)\n",
    "\n",
    "\n",
    "# Build subworkflows and derives graph\n",
    "subworkflows = {name : build(name, subtask) \n",
    "                for (name, subtask) \n",
    "                in workflow_definition.subtasks.items()}\n",
    "\n",
    "subworkflow_graphs = {name : build_graph(subworkflow)\n",
    "                      for (name, subworkflow)\n",
    "                      in subworkflows.items()}\n",
    "\n",
    "# Render SubDags from subworkflow graph\n",
    "render_subtask = partial(render, templater, 'subtask.txt')\n",
    "render_subtasks = {name : render_subtask(subtask.todict())\n",
    "                   for (name, subworkflow_graph) in subworkflow_graphs.items()\n",
    "                   for subtask in topological_sort(subworkflow_graph)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, subtask in render_subtasks.items():\n",
    "    print(subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition.subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dag_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
